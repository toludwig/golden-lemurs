A Convolutional Net (CNN) for text analysis
===========================================

Convolutional neural networks are actually known to be very successful
visual classifiers. However, they can also be applied in Computational
Linguistics for semantic analysis of text. In our case, we want to
learn the the topic of a Repository by 'reading' its README.

Background of convolutions in Computer Vision
---------------------------------------------

If you want to perform object recognition in visual images, probably
you want to be __invariant__ with respect to the size, orientation and the location
of the object within the image. Also it makes (biologically) sense to
start from recognized edges and abstract (__compose__) to shapes and objects.

All these are characteristic functions of **CNN**s  and are achieved by so-called
**convolutions** and **pooling**. A convolution is
basically a filter (aka. kernel) sliding over a whole image, like in the
following nice simulation [^1]:
![picture source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution](./Convolution_schematic.gif)

By applying this with several filters you obtain multiple Convolutional Layers
extracting different features of the image. 
Further location invariance is added by Pooling where some feature
information is thrown away again. For example take max-pooling: there you just take maximum
of your neighbours at each pixel [^2].
![picture source: https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Max_pooling.png](./Max_pooling.png)

While these concepts are very vivid in the case of images, they are not yet for language, let us see...

Word Vectors and the intuition behind them
------------------------------------------

Linguists don't work with images but with corpuses, i.e. huge amounts of real world text.
Why not think of words as vectors (coordinate lists) within the _n_-dimensional corpus space?
Note that the space would be a discrete one, it contains only its words but nothing 'in between'.
However, what we want is a continuous vector space of words. Also, we want it to have
substantially less dimensions (note that _n_ is the size of a typical big data corpus).
We want real-valued vectors, something like:

![picture of vectors in space](./word_vector_space.png)

This reduction of dimensionality can be achieved using modern variants
of the classical bag-of-words model called **word embeddings**.
What is done is basically a packing of cooccurring words in the same 'bag'.
The so obtained vectors show very cool (vector-space) behaviour:
    
    king - man + woman ~ queen
    dog + big + dangerous ~ bulldog

But, lo and behold, there are pretrained datasets around that offer exactly this!
The one we are using is generated by Google using its famous `word2vec` algorithm.
It is trained on the whole **Google News** corpus (_n_=100 billion) and is [available here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit).
    
For further theoretical explanation see [the original paper](http://arxiv.org/pdf/1310.4546.pdf)
by Google's group around Thomas Mikolov.
You may also want to check [this blog post](http://www.foldl.me/2014/glove-python/) on GloVe,
a similiar vector database (which we chose first, but failed to download).

Data preprocessing
------------------
The Google model we are using contains 300-dimensional vectors for 3 million words and phrases.
This means, if you do a lookup for a word in the dataset you will get a list of 300 real numbers
indicating the position of the word within the vector space.

Now, if we want to apply this on a README to extract (hopefully) the topic of the repository,
we need to be aware of two things:
* First, the repo might not have a README at all (or, say it is empty). In this case classification
seems to be futile, but nethertheless it tells us something. We observed, for example that
homework repos often lack a README (for obvious reasons).
  * Solution: we provide an empty vector (filled with 300 zeros) as a dummy for classification.
* Second, texts are cluttered by punctuation signs and other 'text mess'. We don't want to have
this in our learning input, because there are no representations in the database for them,
and they don't carry semantic information.
  * Solution: parse those signs (see `.classification.Data.clean_`)


Our CNN for Word Vectors
------------------------

Now for the actual neural net. We construct a Convolutional Neural Network of multiple
convolutional and pooling layers. Its topology looks like this [^3]:

![picture of cnn topology](./cnn_topology.png)


_________________________

[^1]: picture source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution

[^2]: picture source: https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Max_pooling.png

[^3]: picture source: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/

The latter blog served us as a great tutorial for the whole CNN implementation.
