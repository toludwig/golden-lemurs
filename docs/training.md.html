<h1 id="training-of-the-networks">Training of the networks</h1>
<p>Training of a network means basically adjusting its weights in order to fit it to the function of the inputs to the desired outputs. This way the model <em>learns</em> to predict the classes even for repositories it has never seen before. The only thing we have to do is show the network our sample inputs, let it do its prediction (which is random at the beginning of training) and give it a feedback. This is called <em>Supervised Learning</em>.</p>
<p>This section describes the training of each network which is basically the same procedure for all.</p>
<h2 id="initialising-weights">Initialising weights</h2>
<p>The first thing one has to do to prepare a net for training is to initialize the weights. You could either say, they will be trained anyhow, so let's just set assign them to a constant value (<span class="math inline">\(\neq 0\)</span>, because 0-weights would propagate nothing). But the problem with this is that the backpropagated error will be the same for each weight which makes the model unable to learn.</p>
<p>So what you definitely do is, you take initial weights from a <em>random distribution</em> like a Gaussian (bell curve) or a uniform distribution. In this case you have to specify the range, i.e. a min and a max weight, and commonly you center the distribution at 0, so you have negative weights, too.</p>
<p>Even better is to choose the range for initialization with respect to the size of the layers (<span class="math inline">\(in\)</span> and <span class="math inline">\(out\)</span>)the weight is between. As <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Xavier et. al. suggested in 2010</a>, one should rather use a range of <span class="math inline">\([-x, x]\)</span> with <span class="math inline">\(x = \sqrt{6/(in, out)}\)</span> for uniform distributions or a standard deviation of <span class="math inline">\(\sqrt{3/(in, out)}\)</span> for Gaussian. That's how we do it.</p>
<h2 id="overall-training-procedure-and-learning-batches">Overall training procedure and learning batches</h2>
<p>After initialising the weigths, the network is able to propagate its input towards the output layer. It gets one input of our dataset at a time, one after the other. When the whole samples are seen once, we call this an <strong>epoch</strong>. However, for convergence to the global minimum the net will need many of these epochs. How many, is really a matter of choice, if you are satisfied with the result you can stop training. Of course you should stop earlier if you see that it does not make any learning progress any more, whatever the reason.</p>
<p>During the training we need to provide the feedback. There are different timepoints when you can do this: Either you do it after each sample (Stochastic/online learning) or you accumulate the error for all samples and only update the weights after an epoch is finished. In the first case, the learning will <em>fluctuate</em> very much, in the latter it will be inflexible in terms of reacting differently to different inputs. A good compromise is to use so-called <strong>(mini) batches</strong> that consist of a subset of samples, say e.g. a 10th of the whole dataset. Giving the feeback after each batch makes learning much more stable and efficient. Again, the size of the batches is a value that is individual for each classification and comes from experience. We use batch sizes of <code>200-300</code> samples.</p>
<p>Another preparation concerns ther order of the training samples. They should be learned in a way that each sample is independent from its predecessors, i.e. consequent samples should not be of the same category. Otherwise the network would specicialise into one category too much and does not generalize so easily. This can be prevented by <strong>shuffling</strong> the data before training.</p>
<h2 id="defining-a-loss-function">Defining a Loss function</h2>
<p>As stated above, Supervised Learning is based on giving the network a feedback. This feedback is proportional to the <em>error</em> it makes for each sample <span class="math inline">\(x \in X\)</span>, so we need to define an error function, aka. a <em>Loss function</em>. It is supposed to measure the 'distance' between the generated output <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y \in Y\)</span>, the actual output, in our case the real category of repo <span class="math inline">\(x\)</span>. Let's assume there are <span class="math inline">\(N\)</span> training samples in the following. There are different measurements in use, for example the <em>root mean square error</em> (RSME) defined for numeric classes as follows: <span class="math display">\[RMSE(Y, \hat{Y}) = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2}\]</span></p>
<p>The one we are using is a bit different because it does not make too much sense to substract class labels (even if we encoded them numerically). Instead we use the so-called <strong>Cross-entropy loss</strong> defined like this: <span class="math display">\[CEL(Y, \hat{Y}) = \frac{1}{N} \sum_{i=1}^N H(y_i, \hat{y}_i)\]</span></p>
<p>where <span class="math inline">\(H\)</span> is the cross entropy which one can think of as the probability for a sample of beeing in class <span class="math inline">\(\hat{Y}\)</span> while actually beeing in class <span class="math inline">\(Y\)</span> (here for one-hot encoded outputs): <span class="math display">\[H(Y, \hat{Y}) = - \sum_{y \in Y} y \log \hat{y}\]</span></p>
<h2 id="gradient-descent-and-backpropagation">Gradient descent and Backpropagation</h2>
<p>Now that we have an loss function, we want to minimize the error, i.e. we want to find a (hopefully) global minimum on its surface [<a href="http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20%5B16%5D.png">1</a>]: <img src="/assets/docs/img/gradient_descent.png" alt="picture of gradient descent" /></p>
<p>This is done by adjusting the weights (in the image there are only two, depicted as <span class="math inline">\(\theta_i\)</span>) in a way that leads us to the valleys of the function. This in turn can be achieved by calculating the <strong>gradient</strong> of the loss function, i.e. the derivative with respect to each weigth. Intuitively, one can think of the gradient as a vector pointing to the steepest direction of the error surface. Because we want to 'go <em>down</em> the hill', we take the negative derivative: <span class="math display">\[w_{new} = w - \eta \cdot \frac{\partial CEL}{\partial w}\]</span></p>
<p><span class="math inline">\(\eta\)</span> is a proportional factor called the <strong>learning rate</strong>, a value that we can think of as constant for now.</p>
<p>We do this update for all the weights starting at the last weight layer. The layers before are a little bit more complex to update, but there is an algorithm for that: <strong>Backpropagation of Error</strong> or just BP. As the name suggests, the updates are performed from the output layer backwards, again by differentiating. This envolves repeated use of the <em>chain-rule</em> over all the activation functions until one arrives at the desired weight. Suppose, we have the following chain of neurons <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> in the net... <img src="/assets/docs/img/backpropagation.png" alt="picture of backpropagation" /></p>
<p>In this situation, you would calculate the update <span class="math inline">\(\Delta w_{i,j}\)</span> by: <span class="math display">\[\Delta w_{i,j} = \frac{\partial CEL}{\partial \sigma(k)} \cdot \frac{\partial \sigma(k)}{\partial relu(j)} \cdot \frac{\partial relu(j)}{\partial relu(i)} \cdot \frac{\partial relu(i)}{\partial w_{i,j}}\]</span></p>
<p>Luckily this is done by the Tensorflow framework for us ;)</p>
<h2 id="learning-rate-decay-and-the-adam-optimizer">Learning rate decay and the ADAM optimizer</h2>
<p>The real difficulty with gradient descent approaches is to find a good global minimum. This does depend on your choice of the learning rate. If you choose it too small, gradients will be weighted very low. That means, if you are on a flat place you will get stuck eventually. On the other hand, if <span class="math inline">\(\eta\)</span> is too high, you would do big jumps on the error surface and may end up jumping back and forth over a valley. The best of the two worlds is to make <span class="math inline">\(\eta\)</span> dependent on the training cycle, i.e. decrease it over time. This resembles the way humans would learn, namely beginning in big steps and later doing the fine tuning. In our implementation we use a starting <span class="math inline">\(\eta_0\)</span> of <code>0.001</code> and an exponential decay factor of <code>0.999</code>.</p>
<p>Another way of optimizing gradient descent is to consider gradients from earlier training cycles. This is useful if you have sparse data and some of the inputs are very rare. Then you might want to perform a bigger update for infrequently used weights. An algorithm can do this by storing the average of the last updates per weight.</p>
<p>Both is done by the <strong>ADAM</strong> (Adaptive Moment Estimation) optimizer. For more information see <a href="https://arxiv.org/abs/1412.6980">original paper</a>.</p>
<h2 id="further-tricks-we-use">Further tricks we use</h2>
<p>Another method for tweaking gradients is so-called <strong>gradient clipping</strong>. It is a rather rigorous approach that cuts of gradients, that exceed an upper limit (in our case a value of <code>5.0</code>). It has the purpose to prevent <em>exploding gradients</em>.</p>
<p>Also we define a <strong>dropout-probability</strong>, that is a probability allowing the network to <em>ignore</em> a neuron during training. The problem with large nets is that they tend to overfit and, in fact, only train parts of its structure. A dropout forces the net to use all neurons in order to maintain plasticity. We choose dropout-to-keep a ratio of <code>0.5</code>.</p>
<p>Last but not least, we use so-called <strong>L2 regularization</strong>. This is another way to prevent overfitting and urging the network to have small weights. It uses the mean squared error of all current weights and substracts it from each weight as a penalty, independently from the loss function.</p>
<p><a href="/docs/ensemble">Previous page: Ensemble network</a><br />
<a href="/docs/results">Next page: Our results</a></p>
