Convolutional Neural Networks (CNNs)
====================================

Convolutional neural networks are actually known to be very successful
visual classifiers. However, they can also be applied in Computational
Linguistics for semantic analysis of text. In our case, we want to
learn the the topic of a Repository by 'reading' its README.


Background of convolutions in Computer Vision
---------------------------------------------

If you want to perform object recognition in visual images, probably
you want to be __invariant__ with respect to the size, orientation and the location
of the object within the image. Also it makes (biologically) sense to
start from recognized edges and abstract (__compose__) to shapes and objects.

All these are characteristic functions of **CNN**s  and are achieved by so-called
**convolutions** and **pooling**. A convolution is
basically a filter (aka. kernel) sliding over a whole image, like in the
following nice simulation [[1]]:

![picture of convolution](assets/docs/img/Convolution_schematic.gif)

By applying this with several filters you obtain multiple Convolutional Layers
extracting different features of the image. 
Further location invariance is added by Pooling where some feature
information is thrown away again. For example take max-pooling: there you just take maximum
of your neighbours at each pixel [[2]].

![picture of pooling](assets/docs/img/Max_pooling.png)

While these concepts are very vivid in the case of images, they are not yet for language, let us see...


Word Vectors and the intuition behind them
------------------------------------------

Linguists don't work with images but with corpuses, i.e. huge amounts of real world text.
Why not think of words as vectors (coordinate lists) within the _n_-dimensional corpus space?
Note that the space would be a discrete one, it contains only its words but nothing 'in between'.
However, what we want is a continuous vector space of words. Also, we want it to have
substantially less dimensions (note that _n_ is the size of a typical big data corpus).
We want real-valued vectors, something like:

![picture of vectors in space](assets/docs/img/word_vector_space.png)

This reduction of dimensionality can be achieved using modern variants
of the classical bag-of-words model called **word embeddings**.
What is done is basically a packing of cooccurring words in the same 'bag'.
The so obtained vectors show very cool (vector-space) behaviour:
    
    king - man + woman ~ queen
    dog + big + dangerous ~ bulldog

But, lo and behold, there are pretrained datasets around that offer exactly this!
The one we are using is generated by Google using its famous *word2vec* algorithm.
It is trained on the whole **Google News** corpus (_n_=100 billion) and is [available here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit).
The model contains 300-dimensional vectors for 3 million words and phrases.
This means, if you do a lookup for a word in the dataset you will get a list of 300 real numbers
indicating the position of the word within the vector space.
    
For further theoretical explanation see [the original paper](http://arxiv.org/pdf/1310.4546.pdf)
by Google's group around Thomas Mikolov.
You may also want to check [this Blog post](http://www.foldl.me/2014/glove-python/) on GloVe,
a similiar vector database (which we chose first, but failed to download).

Data preprocessing
------------------

We work with the first `300` words of the README and the first `400` words of the
commits (concatenated from multiple commits). If either of both texts is not long
enough, we fill the vector with padding words (`\pad\`).

Now, if we want to apply this to extract (hopefully) the topic of the repository,
we need to be aware of two things:

1. First, the repo might not have a README at all (or, say it is empty). In this case classification
seems to be futile, but nethertheless it tells us something. We observed for example that
homework repos often lack a README (for obvious reasons).
    * Solution: we provide an empty vector (filled with 300 pads) as a dummy for classification.

2. Second, texts are cluttered by punctuation signs and other 'text mess'. After we applied
the word tokenizer to the texts, these are still sticking at the words. However,
there are word-vectors for those signs so we do not need/want to extract them entirely.
    * Solution: parse and split those signs from the words (see `classification.Data.clean_str`)


Our CNN for Word Vectors
------------------------
Now for the actual neural net. We construct a Convolutional Neural Network of a single
convolutional and following pooling layer. Its topology looks like this [[3]]:

![picture of cnn topology](assets/docs/img/cnn_topology.png)

The size of the input matrix is 300 words of the mentioned vector dimensionality 300.

For the convolutional layer, we use one-dimensional filters over multiple words
with sizes of _k_ out of `[3, 4, 5]`.
For each filter we have `200` instances of initially random weights which are learned in the training.

The strides with which the filters are sliding equal `1` for all filter sizes,
hence they are of course overlapping.

After we applied these convolutions one layer of max pooling follows.

Then we have hopefully all the features extracted and can learn on those.
This is done by the last two fully connected layers.


Our CNN for Commit messages
---------------------------
The CNN for Commit messages looks basically the same with the only difference
that the input size is 400 words.


[1]: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution

[2]: https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Max_pooling.png

[3]: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/
