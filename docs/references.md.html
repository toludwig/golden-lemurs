<h1 id="list-of-referenced-papers">List of Referenced Papers</h1>
<ul>
<li><p>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1â€“13.</p></li>
<li><p>Xavier Glorot and Yoshua Bengio (2010): Understanding the difficulty of training deep feedforward neural networks. International conference on artificial intelligence and statistics.</p></li>
</ul>
<h1 id="list-of-useful-blogs">List of useful Blogs</h1>
<ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><strong>colah's</strong> blog on LSTMs</a></li>
<li><a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"><strong>WildML</strong> on CNNs for Text</a></li>
<li><a href="http://www.foldl.me/2014/glove-python/"><strong>foldl</strong> on GloVe</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/index.html"><strong>Sebastian Ruder</strong> on gradient descent</a></li>
</ul>
<p><a href="/docs/discussion">Previous page: Discussion</a><br />
<a href="/docs/intro">Table of Contents</a></p>
